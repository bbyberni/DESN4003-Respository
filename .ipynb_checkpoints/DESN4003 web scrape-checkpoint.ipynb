{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "146eab96-d9dd-4224-8c65-29ecf1c79b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Google block check...\n",
      "‚ùå BLOCKED: Google detected scraping (HTTP 403)\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF YOU ARE STILL BLOCKED BY GOOGLE 12PM APR 3 (1-24 HOURS IF TEMPORARY, SEVERAL DAYS TO WEEKS IF AGGRESSIVE, RARE PERMANENT BLOCK)\n",
    "# SEE IF YOU CAN USE THE SAME API KEY LATER ON, IF NOT GENERATE A NEW ONE WITH SCRAPINGDOG (200 API KEYS AVAILABLE IN THE NEXT MONTH)\n",
    "# 1 API KEY CAN SCRAPE ~4300 RESULTS BEFORE GETTING BLOCKED\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"67e973fad9a23c3a9fd5e970\"  # Replace with your actual key\n",
    "BASE_URL = \"https://api.scrapingdog.com/google\"\n",
    "TEST_QUERY = \"weather today\"  # Generic test query\n",
    "\n",
    "def check_google_block():\n",
    "    \"\"\"Check if Google is blocking the API key with a minimal test request\"\"\"\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"query\": TEST_QUERY,\n",
    "        \"results\": 1,  # Only 1 result to reduce footprint\n",
    "        \"country\": \"us\",\n",
    "        \"advance_search\": \"false\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Add slight random delay\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        response = requests.get(BASE_URL, params=params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('organic_results'):\n",
    "                print(\"‚úÖ Success! You're NOT blocked. Sample result:\")\n",
    "                print(f\"Title: {data['organic_results'][0]['title']}\")\n",
    "                print(f\"Link: {data['organic_results'][0]['link']}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è API working but no results returned (possible soft block)\")\n",
    "        elif response.status_code == 403:\n",
    "            print(\"‚ùå BLOCKED: Google detected scraping (HTTP 403)\")\n",
    "        elif response.status_code == 429:\n",
    "            print(\"‚è≥ RATE LIMITED: Too many requests (HTTP 429)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected status: {response.status_code} | Response: {response.text[:200]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Connection failed: {str(e)}\")\n",
    "        print(\"This could indicate a block or network issue\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running Google block check...\")\n",
    "    check_google_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed27bab-66e6-43a0-b462-804607cff00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Google searches for 15 queries\n",
      "\n",
      "Searching for: 'climate database'\n",
      "Page 1: 10 results | 10 new\n",
      "Page 2: 99 results | 88 new\n",
      "Page 3: 99 results | 5 new\n",
      "Page 4: 98 results | 1 new\n",
      "Page 5: 88 results | 0 new\n",
      "Page 6: 75 results | 0 new\n",
      "Page 7: 65 results | 0 new\n",
      "Page 8: 53 results | 0 new\n",
      "Page 9: 45 results | 0 new\n",
      "Page 10: 10 results | 0 new\n",
      "\n",
      "Searching for: 'socioeconomic statistics'\n",
      "Page 1: 10 results | 10 new\n",
      "Page 2: 10 results | 8 new\n",
      "Page 3: 99 results | 93 new\n",
      "Page 4: 100 results | 2 new\n",
      "Page 5: 99 results | 1 new\n",
      "Page 6: 87 results | 0 new\n",
      "Page 7: 79 results | 0 new\n",
      "Page 8: 69 results | 0 new\n",
      "Page 9: 59 results | 0 new\n",
      "Page 10: 49 results | 0 new\n",
      "\n",
      "Searching for: 'climate vulnerability assessment'\n",
      "Page 1: 99 results | 93 new\n",
      "Page 2: 99 results | 5 new\n",
      "Page 3: 99 results | 8 new\n",
      "Page 4: 99 results | 14 new\n",
      "Page 5: 99 results | 8 new\n",
      "Page 6: 99 results | 11 new\n",
      "Page 7: 99 results | 8 new\n",
      "Page 8: 99 results | 13 new\n",
      "Page 9: 99 results | 5 new\n",
      "Page 10: 99 results | 1 new\n",
      "\n",
      "Searching for: 'climate risk model'\n",
      "Page 1: 99 results | 91 new\n",
      "Page 2: 99 results | 11 new\n",
      "Page 3: 99 results | 6 new\n",
      "Page 4: 99 results | 9 new\n",
      "Page 5: 99 results | 9 new\n",
      "Page 6: 100 results | 9 new\n",
      "Page 7: 99 results | 8 new\n",
      "Page 8: 100 results | 9 new\n",
      "Page 9: 99 results | 8 new\n",
      "Page 10: 99 results | 3 new\n",
      "\n",
      "Searching for: 'climate fund'\n",
      "Page 1: 96 results | 90 new\n",
      "Page 2: 99 results | 9 new\n",
      "Page 3: 99 results | 9 new\n",
      "Page 4: 99 results | 9 new\n",
      "Page 5: 99 results | 9 new\n",
      "Page 6: 99 results | 4 new\n",
      "Page 7: 99 results | 0 new\n",
      "Page 8: 13 results | 0 new\n",
      "Page 9: 99 results | 4 new\n",
      "Page 10: 13 results | 0 new\n",
      "\n",
      "Searching for: 'climate adaptation framework'\n",
      "Page 1: 99 results | 89 new\n",
      "Page 2: 100 results | 5 new\n",
      "Page 3: 100 results | 5 new\n",
      "Page 4: 100 results | 7 new\n",
      "Page 5: 100 results | 9 new\n",
      "Page 6: 100 results | 11 new\n",
      "Page 7: 98 results | 6 new\n",
      "Page 8: 88 results | 0 new\n",
      "Page 9: 78 results | 0 new\n",
      "Page 10: 68 results | 0 new\n",
      "\n",
      "Searching for: 'climate policy'\n",
      "Page 1: 99 results | 89 new\n",
      "Page 2: 99 results | 11 new\n",
      "Page 3: 99 results | 11 new\n",
      "Page 4: 99 results | 8 new\n",
      "Page 5: 99 results | 8 new\n",
      "Page 6: 13 results | 2 new\n",
      "Page 7: 99 results | 20 new\n",
      "Page 8: 99 results | 9 new\n",
      "Page 9: 99 results | 0 new\n",
      "Page 10: 99 results | 0 new\n",
      "\n",
      "Searching for: 'climate technology'\n",
      "Page 1: 98 results | 88 new\n",
      "Page 2: 100 results | 12 new\n",
      "Page 3: 100 results | 8 new\n",
      "Page 4: 100 results | 8 new\n",
      "Page 5: 13 results | 0 new\n",
      "Page 6: 100 results | 15 new\n",
      "Page 7: 13 results | 0 new\n",
      "Page 8: 100 results | 2 new\n",
      "Page 9: 100 results | 0 new\n",
      "Page 10: 100 results | 0 new\n",
      "\n",
      "Searching for: 'climate innovation research'\n",
      "Page 1: 100 results | 98 new\n",
      "Page 2: 100 results | 11 new\n",
      "Page 3: 100 results | 11 new\n",
      "Page 4: 99 results | 18 new\n",
      "Page 5: 10 results | 0 new\n",
      "Page 6: 100 results | 20 new\n",
      "Page 7: 99 results | 9 new\n",
      "Page 8: 99 results | 10 new\n",
      "Page 9: 99 results | 8 new\n",
      "Page 10: 99 results | 10 new\n",
      "\n",
      "Searching for: 'climate resilience building'\n",
      "Page 1: 10 results | 10 new\n",
      "Page 2: 99 results | 95 new\n",
      "Page 3: 99 results | 19 new\n",
      "Page 4: 99 results | 13 new\n",
      "Page 5: 99 results | 11 new\n",
      "Page 6: 99 results | 10 new\n",
      "Page 7: 99 results | 9 new\n",
      "Page 8: 13 results | 1 new\n",
      "Page 9: 100 results | 20 new\n",
      "Page 10: 100 results | 10 new\n",
      "\n",
      "Searching for: 'climate adaptation and mitigation strategies'\n",
      "Page 1: 99 results | 84 new\n",
      "Page 2: 99 results | 3 new\n",
      "Page 3: 99 results | 4 new\n",
      "Page 4: 99 results | 5 new\n",
      "Page 5: 99 results | 6 new\n",
      "Page 6: 99 results | 6 new\n",
      "Page 7: 10 results | 0 new\n",
      "Page 8: 100 results | 7 new\n",
      "Page 9: 99 results | 2 new\n",
      "Page 10: 100 results | 5 new\n",
      "\n",
      "Searching for: 'climate strategy monitoring'\n",
      "Page 1: 99 results | 88 new\n",
      "Page 2: 10 results | 0 new\n",
      "Page 3: 99 results | 20 new\n",
      "Page 4: 99 results | 12 new\n",
      "Page 5: 99 results | 9 new\n",
      "Page 6: 99 results | 9 new\n",
      "Page 7: 99 results | 9 new\n",
      "Page 8: 99 results | 9 new\n",
      "Page 9: 10 results | 0 new\n",
      "Page 10: 99 results | 17 new\n",
      "\n",
      "Searching for: 'climate strategy evaluation'\n",
      "Page 1: 99 results | 79 new\n",
      "Page 2: 99 results | 9 new\n",
      "Page 3: 99 results | 10 new\n",
      "Page 4: 99 results | 10 new\n",
      "Page 5: 99 results | 8 new\n",
      "Page 6: 10 results | 0 new\n",
      "Page 7: 99 results | 17 new\n",
      "Page 8: 99 results | 13 new\n",
      "Page 9: 99 results | 6 new\n",
      "Page 10: 99 results | 6 new\n",
      "\n",
      "Searching for: 'climate resilience program'\n",
      "Page 1: 99 results | 66 new\n",
      "Page 2: 99 results | 7 new\n",
      "Page 3: 99 results | 9 new\n",
      "Page 4: 99 results | 6 new\n",
      "Page 5: 99 results | 6 new\n",
      "Page 6: 99 results | 6 new\n",
      "Page 7: 99 results | 7 new\n",
      "Page 8: 99 results | 10 new\n",
      "Page 9: 99 results | 8 new\n",
      "Page 10: 99 results | 13 new\n",
      "\n",
      "Searching for: 'climate adaptation plan'\n",
      "Page 1: 99 results | 58 new\n",
      "Page 2: 100 results | 7 new\n",
      "Page 3: 100 results | 3 new\n",
      "Page 4: 100 results | 3 new\n",
      "Page 5: 100 results | 2 new\n",
      "Page 6: 100 results | 2 new\n",
      "Page 7: 100 results | 6 new\n",
      "Page 8: 100 results | 5 new\n",
      "Page 9: 100 results | 7 new\n",
      "Page 10: 100 results | 2 new\n",
      "\n",
      "Success! Saved 2183 unique results to /Users/berni/Desktop/trial1.csv\n"
     ]
    }
   ],
   "source": [
    "# INTERNATIONAL GENERAL (DONE - FILTER IN POST)\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"67e973fad9a23c3a9fd5e970\"\n",
    "BASE_URL = \"https://api.scrapingdog.com/google\"\n",
    "OUTPUT_FILE = os.path.expanduser(\"~/Desktop/trial1.csv\")\n",
    "QUERIES = [\"climate database\",\n",
    "         \"socioeconomic statistics\",\n",
    "         \"climate vulnerability assessment\",\n",
    "         \"climate risk model\",\n",
    "         \"climate fund\",\n",
    "         \"climate adaptation framework\",\n",
    "         \"climate policy\",\n",
    "         \"climate technology\",\n",
    "         \"climate innovation research\",\n",
    "        \"climate resilience building\",\n",
    "        \"climate adaptation and mitigation strategies\",\n",
    "        \"climate strategy monitoring\",\n",
    "        \"climate strategy evaluation\",\n",
    "        \"climate resilience program\",\n",
    "        \"climate adaptation plan\"]\n",
    "\n",
    "def is_duplicate(result, existing_urls, existing_titles):\n",
    "    \"\"\"Check if result is a duplicate based on URL and title\"\"\"\n",
    "    url = result.get('link', '').lower().strip()\n",
    "    title = result.get('title', '').lower().strip()\n",
    "    \n",
    "    # Remove common tracking parameters from URLs\n",
    "    clean_url = url.split('?')[0].split('#')[0]\n",
    "    \n",
    "    return (clean_url in existing_urls) or (title in existing_titles)\n",
    "\n",
    "def fetch_and_save_results():\n",
    "    \"\"\"Fetch Google results, remove duplicates, and save to CSV\"\"\"\n",
    "    all_results = []\n",
    "    seen_urls = set()\n",
    "    seen_titles = set()\n",
    "    \n",
    "    for query in QUERIES:\n",
    "        print(f\"\\nSearching for: '{query}'\")\n",
    "        \n",
    "        for page in range(10):  # Pages 0-9\n",
    "            params = {\n",
    "                \"api_key\": API_KEY,\n",
    "                \"query\": query,\n",
    "                \"results\": 100,\n",
    "                \"country\": \"us\",\n",
    "                \"page\": page,\n",
    "                \"advance_search\": \"false\"\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                organic_results = data.get('organic_results', [])\n",
    "                \n",
    "                new_results = 0\n",
    "                for result in organic_results:\n",
    "                    if not is_duplicate(result, seen_urls, seen_titles):\n",
    "                        url = result.get('link', '').lower().strip()\n",
    "                        clean_url = url.split('?')[0].split('#')[0]\n",
    "                        title = result.get('title', '').lower().strip()\n",
    "                        \n",
    "                        seen_urls.add(clean_url)\n",
    "                        seen_titles.add(title)\n",
    "                        \n",
    "                        all_results.append({\n",
    "                            'query': query,\n",
    "                            'rank': result.get('rank'),\n",
    "                            'title': result.get('title', '').strip(),\n",
    "                            'link': url,\n",
    "                            'snippet': result.get('snippet', '').replace('\\n', ' ').strip(),\n",
    "                            'page': page + 1\n",
    "                        })\n",
    "                        new_results += 1\n",
    "                \n",
    "                print(f\"Page {page + 1}: {len(organic_results)} results | {new_results} new\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page + 1}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Save to CSV\n",
    "    if all_results:\n",
    "        fieldnames = ['query', 'rank', 'title', 'link', 'snippet', 'page']\n",
    "        \n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_results)\n",
    "            \n",
    "        print(f\"\\nSuccess! Saved {len(all_results)} unique results to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No results to save\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting Google searches for {len(QUERIES)} queries\")\n",
    "    fetch_and_save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da546dfa-baba-4eb4-a203-89b8cbe36ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTRY-SPECIFIC (RE-TRY LATER - ADDED TIME DELAY AND HEADERS, CONSIDER ROTATING PROXIES (VPN?) AND HEADERS)\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"67e973fad9a23c3a9fd5e970\"\n",
    "BASE_URL = \"https://api.scrapingdog.com/google\"\n",
    "OUTPUT_FILE = os.path.expanduser(\"~/Desktop/countrytrial.csv\")\n",
    "QUERIES = [\"administrative service\",\n",
    "          \"natural disaster\",\n",
    "          \"disaster relief\",\n",
    "          \"aid delivery\",\n",
    "          \"government protection\",\n",
    "          \"capacity building program\",\n",
    "          \"resilience training\",\n",
    "          \"local adaptation efforts\",\n",
    "          \"municipal policy\",\n",
    "          \"weather warning\",\n",
    "          \"rural improvement\",\n",
    "          \"regional law\",\n",
    "          \"provincial office\",\n",
    "          \"official statistics\",\n",
    "          \"climate report\"]\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9,no;q=0.8\",\n",
    "    \"cache-control\": \"no-cache\",\n",
    "    \"cookie\": \"SOCS=CAESHAgCEhJnd3NfMjAyNDEwMDMtMF9SQzEaAmVuIAEaBgiApoe4Bg; __Secure-BUCKET=CLQC; SEARCH_SAMESITE=CgQIkJ0B; HSID=Ax0y33Enje0UKSJPo; SSID=A9cmbWiDlI2OBuHZT; APISID=nmYz1QxsJhP0g1yR/A8YqUSUkhm8IcFxGD; SAPISID=6QHzUcfRrMWlUVCG/AKD4SutBCBcM2N3xH; __Secure-1PAPISID=6QHzUcfRrMWlUVCG/AKD4SutBCBcM2N3xH; __Secure-3PAPISID=6QHzUcfRrMWlUVCG/AKD4SutBCBcM2N3xH; SID=g.a000uwh39SaBgDO170HyHlwrmugxi6wvHndyxPCjS_ut2oRv927o-QeAshD48HBGD92aT_C_TQACgYKAdASARUSFQHGX2MiT37QypurPXs1oOw3M51yRhoVAUF8yKo7Xvam4trmY_aMLdPVbcsD0076; __Secure-1PSID=g.a000uwh39SaBgDO170HyHlwrmugxi6wvHndyxPCjS_ut2oRv927oUvFohrciKEyUg7DiNALqwAACgYKAbkSARUSFQHGX2MiwLnWzwSKLFK6iAh_rZbq-xoVAUF8yKq_z6bURjNo9Z2OskbNB3_P0076; __Secure-3PSID=g.a000uwh39SaBgDO170HyHlwrmugxi6wvHndyxPCjS_ut2oRv927oLCDIgsbUCdVXnbsMRBxXkAACgYKAbASARUSFQHGX2MiQDiuB3M1f7BOoWkl1n8p8BoVAUF8yKr03VED9j6Mgg3S-yp-czle0076; AEC=AVcja2duJw0wPzXr5MorDrVVUvWqLIiaKyJHAkYGTms9mPHCbHNjVNu8mSw; NID=523=SaP1Qxjr4bbID-1wRVZYKVJRTByaFeOQDRC9EkEDGsx95nfSyjuh1djU89nA0Ajqj1l0z5wBuyLLn63t1qfkq1MjeB1eg4shmURhNRh1Ch9EjqrG_SSo0U2L59ISd85RoiSssz_J3PMqQNwYqZ2KQ5ugGDVZm-e54iMNecWK_JpzMm3GJPNRJWm1BBzD4-mkXi5QR5KLpDAa9QdpE7AOVCIN3Y0NH3UK-Tik8P-KyGe94IImybvh_kZsO3thwDu-W4TQ2aUlHgAO2L6cVTGtsvvgkdiKbobphI6vfHoxIIr_9TSBQB-l3q4epzTrnvL8n0UemNupW3bjrYLlReFGepyl0kNSKBsdcn7OnUvI31BkKI6gjx7n8g1HfydzG-hdzJEuqWlWm0NajT9rPTK9fXJcbqo-WCSCKPPZPjfuY90_TJ2dlxGA5XzcVGmKK-n5d6ANDeD_19viw6R4hkYntzJEKEApSt7ew_sO0uRlVepGjlhYZdXldIvkzysZ9Xv3Dwm2aWjPJSNb0WJGU3gTWi4VDjcG4ysWpLXSnIUCuBp6LvaPEr_4xPYmoiBbQN81r-f8H0F74qG-djDAQ_dfGe2b7Mtbw76Fmq_O7eQpayt4AYOCZW_QxnB-aoj2Odx5EYMUrN7rDhuReHzWYwFWbwIGpCkN-3_SMpVvMGA8aX6TcFv-7pztsS5zA00uV_Vh6Dmfrz8e9ZtvDBSnZhhOPc2gccIuqcWNYcXBYvn3gYSMQbVw5eLUoVDQpYFjib1whN-ZqMteYIUXLcc7cXzvVApk2pX_ZAKd8fFcPJKCj4jWrOUdsTZD3X-A5ki-wHEra2_Ohp74MBJADjBjcjzYJEpFjokLRioshwNk4iZGiw-kN8h1SK86X8UbZ91mMmu_jXLv9dJdKqLxvELHzibCHBS2LehyHAEYolRZKU9YFCgBnViPwrTsVcDKh1aZglkQ8y5Y5235Y4kYEmgqNhrooke3oNxBRQNMRYgOYCsGZ4eZmtbsQgVJtzSD-14GI7_zAPn-NqNKJ_GnEg_nUOqqvXPbymSX1obZgCPT6KT1JzgqJ1Vjrwoef3gt_ZbmuqGNXkw1AT4Zm8eJSc9ySvPZo2UAFZEplhTmyq88xuE702gm7FG4dE2tJkNPLsLRstEt7pggJ6TgTZyuujubGGobU28i3YZEbTeUa5ufINPJxIdOLCZp6GL375JQqeE_NHMeT4AITrJ7oJKjdl3onyelo8LccXxMAgc2uVSU8T-3FclpU6eJNi8j2cSpBncea57bLvkDZDQ7qWoD720IX-JLGqtWcZdIRsnbTaBUOII3kC3s_n-Y; __Secure-ENID=26.SE=IfiJg-J4k8y7GloTPrsBR5Vq2_M4OcuKozjSaqNFQ2WJWTweIWt7Df9-cDo2wVnwE_-yKDYPgZdF1EA_qa4R81T9CdqrZvsmOMIXsBnVjcInmQFmwApIYRm9yJPv2-W-tOxz0KKkpWHHIk5B1kacdEllPQBzypOVeEZaHKAhLlgY5rnmyaoXU99fQGZDgL_yUpzCY_WfBapW9r02zEElQ4nieSV8ggKDY1f_U4q5MmAh1r0FC0f2syGbdts4CPRi0vmPCGM6nCS0-QiVhENBlSkF5KAFh0YTEOhPpsNmBLDtZLA5VwTndNXYuTIgmEzygMs6w5CtaxMYuzluf64czg1QRG12V6SySbT5EKcLlsKVQAkI-1uXrev_BP5i-E2p3AczK1kMyCSSRDBjqExVErZsIsY1KzeqpNsR3S8gLDGPogwfuO_1ZoV98aIJjEmodmhGjf5HKhdLjpgoOxj1kgdijc9PRVfK8QiMam398hFAjyjcnFToIFYYA-O82Gh8Wcqcz1PUnQ1XPQ; __Secure-1PSIDTS=sidts-CjEB7pHptfiZGfvho9KoBxhqz5cRBbIQV2YPYmkirnmWLht9FsmEcM2rTVjAuqVfwTWHEAA; __Secure-3PSIDTS=sidts-CjEB7pHptfiZGfvho9KoBxhqz5cRBbIQV2YPYmkirnmWLht9FsmEcM2rTVjAuqVfwTWHEAA; DV=AxyFY00EA5pT4B5gABCykxHlr4eeX1mAuClUv9L13QIAAFBxZ8UCkzF_zwAAAKBhEoxfPqk8SAAAAOFrxYOZiv9pEwAAAA; SIDCC=AKEyXzU6-HsLQTEeVBFjTFUuUNDWtXdiKq1lsWuhtq-cuaMjQWKwcLxT9UyRzGiGrx2a7qTSuQ; __Secure-1PSIDCC=AKEyXzXsqCX_Wr_o-nNRolEUDe-kreVtK5dYi9p1SsH06-TSqvLQ613PWRBz3VwMRTW8CMnwco0; __Secure-3PSIDCC=AKEyXzUsZJl3foJ_h4i8oAVPllEvA4nSInVkCcaXl9iUSGUqhi0gtLAcni3QM9imwnTjR5YwYRQ\",\n",
    "    \"downlink\": \"10\",\n",
    "    \"pragma\": \"no-cache\",\n",
    "    \"priority\": \"u=1, i\",\n",
    "    \"referer\": \"https://www.google.com/\",\n",
    "    \"rtt\": \"50\",\n",
    "    \"sec-ch-prefers-color-scheme\": \"dark\",\n",
    "    \"sec-ch-ua\": \"\\\"Chromium\\\";v=\\\"134\\\", \\\"Not:A-Brand\\\";v=\\\"24\\\", \\\"Google Chrome\\\";v=\\\"134\\\"\",\n",
    "    \"sec-ch-ua-arch\": \"\\\"arm\\\"\",\n",
    "    \"sec-ch-ua-bitness\": \"\\\"64\\\"\",\n",
    "    \"sec-ch-ua-form-factors\": \"\\\"Desktop\\\"\",\n",
    "    \"sec-ch-ua-full-version\": \"\\\"134.0.6998.166\\\"\",\n",
    "    \"sec-ch-ua-full-version-list\": \"\\\"Chromium\\\";v=\\\"134.0.6998.166\\\", \\\"Not:A-Brand\\\";v=\\\"24.0.0.0\\\", \\\"Google Chrome\\\";v=\\\"134.0.6998.166\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-model\": \"\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",\n",
    "    \"sec-ch-ua-platform-version\": \"\\\"14.5.0\\\"\",\n",
    "    \"sec-ch-ua-wow64\": \"?0\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\",\n",
    "    \"x-client-data\": \"CIa2yQEIpbbJAQipncoBCJ7hygEIlaHLAQiKo8sBCIagzQEIuMjNAQj+pc4BCLzVzgEIruTOARjh4s4B\"\n",
    "}\n",
    "\n",
    "def is_duplicate(result, existing_urls, existing_titles):\n",
    "    \"\"\"Check if result is a duplicate based on URL and title\"\"\"\n",
    "    url = result.get('link', '').lower().strip()\n",
    "    title = result.get('title', '').lower().strip()\n",
    "    \n",
    "    # Remove common tracking parameters from URLs\n",
    "    clean_url = url.split('?')[0].split('#')[0]\n",
    "    \n",
    "    return (clean_url in existing_urls) or (title in existing_titles)\n",
    "\n",
    "def fetch_and_save_results():\n",
    "    \"\"\"Fetch Google results, remove duplicates, and save to CSV\"\"\"\n",
    "    all_results = []\n",
    "    seen_urls = set()\n",
    "    seen_titles = set()\n",
    "    \n",
    "    for query in QUERIES:\n",
    "        print(f\"\\nSearching for: '{query}'\")\n",
    "        \n",
    "        for page in range(10):  # Pages 0-9\n",
    "\n",
    "            delay = random.uniform(2, 5)  \n",
    "            time.sleep(delay)\n",
    "            \n",
    "            params = {\n",
    "                \"api_key\": API_KEY,\n",
    "                \"query\": query,\n",
    "                \"results\": 100,\n",
    "                \"country\": \"ph\",\n",
    "                \"page\": page,\n",
    "                \"advance_search\": \"false\"\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params, headers=headers, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                organic_results = data.get('organic_results', [])\n",
    "                \n",
    "                new_results = 0\n",
    "                for result in organic_results:\n",
    "                    if not is_duplicate(result, seen_urls, seen_titles):\n",
    "                        url = result.get('link', '').lower().strip()\n",
    "                        clean_url = url.split('?')[0].split('#')[0]\n",
    "                        title = result.get('title', '').lower().strip()\n",
    "                        \n",
    "                        seen_urls.add(clean_url)\n",
    "                        seen_titles.add(title)\n",
    "                        \n",
    "                        all_results.append({\n",
    "                            'query': query,\n",
    "                            'rank': result.get('rank'),\n",
    "                            'title': result.get('title', '').strip(),\n",
    "                            'link': url,\n",
    "                            'snippet': result.get('snippet', '').replace('\\n', ' ').strip(),\n",
    "                            'page': page + 1\n",
    "                        })\n",
    "                        new_results += 1\n",
    "                \n",
    "                print(f\"Page {page + 1}: {len(organic_results)} results | {new_results} new\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page + 1}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Save to CSV\n",
    "    if all_results:\n",
    "        fieldnames = ['query', 'rank', 'title', 'link', 'snippet', 'page']\n",
    "        \n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_results)\n",
    "            \n",
    "        print(f\"\\nSuccess! Saved {len(all_results)} unique results to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No results to save\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting Google searches for {len(QUERIES)} queries\")\n",
    "    fetch_and_save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
