{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146eab96-d9dd-4224-8c65-29ecf1c79b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Google block check...\n",
      "‚ùå BLOCKED: Google detected scraping (HTTP 403)\n"
     ]
    }
   ],
   "source": [
    "# CHECK IF YOU ARE STILL BLOCKED BY GOOGLE 12PM APR 3 (1-24 HOURS IF TEMPORARY, SEVERAL DAYS TO WEEKS IF AGGRESSIVE, RARE PERMANENT BLOCK)\n",
    "# SEE IF YOU CAN USE THE SAME API KEY LATER ON, IF NOT GENERATE A NEW ONE WITH SCRAPINGDOG (200 API KEYS AVAILABLE IN THE NEXT MONTH)\n",
    "# 1 API KEY CAN SCRAPE ~4300 RESULTS BEFORE GETTING BLOCKED\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"67e973fad9a23c3a9fd5e970\"  # Replace with your actual key\n",
    "BASE_URL = \"https://api.scrapingdog.com/google\"\n",
    "TEST_QUERY = \"weather today\"  # Generic test query\n",
    "\n",
    "def check_google_block():\n",
    "    \"\"\"Check if Google is blocking the API key with a minimal test request\"\"\"\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"query\": TEST_QUERY,\n",
    "        \"results\": 1,  # Only 1 result to reduce footprint\n",
    "        \"country\": \"us\",\n",
    "        \"advance_search\": \"false\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Add slight random delay\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        response = requests.get(BASE_URL, params=params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('organic_results'):\n",
    "                print(\"‚úÖ Success! You're NOT blocked. Sample result:\")\n",
    "                print(f\"Title: {data['organic_results'][0]['title']}\")\n",
    "                print(f\"Link: {data['organic_results'][0]['link']}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è API working but no results returned (possible soft block)\")\n",
    "        elif response.status_code == 403:\n",
    "            print(\"‚ùå BLOCKED: Google detected scraping (HTTP 403)\")\n",
    "        elif response.status_code == 429:\n",
    "            print(\"‚è≥ RATE LIMITED: Too many requests (HTTP 429)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected status: {response.status_code} | Response: {response.text[:200]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Connection failed: {str(e)}\")\n",
    "        print(\"This could indicate a block or network issue\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running Google block check...\")\n",
    "    check_google_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a9a3cf-9215-43ce-993d-c7644cee0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f55757-5ec6-4fc7-b7e1-a289a96e0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERNATIONAL GENERAL (DONE - FILTER IN POST)\n",
    "\n",
    "# Configuration\n",
    "API_KEY = \"67e973fad9a23c3a9fd5e970\"\n",
    "BASE_URL = \"https://api.scrapingdog.com/google\"\n",
    "OUTPUT_FILE = os.path.expanduser(\"~/Desktop/trial1.csv\")\n",
    "QUERIES = [\"climate database\",\n",
    "         \"socioeconomic statistics\",\n",
    "         \"climate vulnerability assessment\",\n",
    "         \"climate risk model\",\n",
    "         \"climate fund\",\n",
    "         \"climate adaptation framework\",\n",
    "         \"climate policy\",\n",
    "         \"climate technology\",\n",
    "         \"climate innovation research\",\n",
    "        \"climate resilience building\",\n",
    "        \"climate adaptation and mitigation strategies\",\n",
    "        \"climate strategy monitoring\",\n",
    "        \"climate strategy evaluation\",\n",
    "        \"climate resilience program\",\n",
    "        \"climate adaptation plan\"]\n",
    "\n",
    "def is_duplicate(result, existing_urls, existing_titles):\n",
    "    \"\"\"Check if result is a duplicate based on URL and title\"\"\"\n",
    "    url = result.get('link', '').lower().strip()\n",
    "    title = result.get('title', '').lower().strip()\n",
    "    \n",
    "    # Remove common tracking parameters from URLs\n",
    "    clean_url = url.split('?')[0].split('#')[0]\n",
    "    \n",
    "    return (clean_url in existing_urls) or (title in existing_titles)\n",
    "\n",
    "def fetch_and_save_results():\n",
    "    \"\"\"Fetch Google results, remove duplicates, and save to CSV\"\"\"\n",
    "    all_results = []\n",
    "    seen_urls = set()\n",
    "    seen_titles = set()\n",
    "    \n",
    "    for query in QUERIES:\n",
    "        print(f\"\\nSearching for: '{query}'\")\n",
    "        \n",
    "        for page in range(10):  # Pages 0-9\n",
    "            params = {\n",
    "                \"api_key\": API_KEY,\n",
    "                \"query\": query,\n",
    "                \"results\": 100,\n",
    "                \"country\": \"us\",\n",
    "                \"page\": page,\n",
    "                \"advance_search\": \"false\"\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                organic_results = data.get('organic_results', [])\n",
    "                \n",
    "                new_results = 0\n",
    "                for result in organic_results:\n",
    "                    if not is_duplicate(result, seen_urls, seen_titles):\n",
    "                        url = result.get('link', '').lower().strip()\n",
    "                        clean_url = url.split('?')[0].split('#')[0]\n",
    "                        title = result.get('title', '').lower().strip()\n",
    "                        \n",
    "                        seen_urls.add(clean_url)\n",
    "                        seen_titles.add(title)\n",
    "                        \n",
    "                        all_results.append({\n",
    "                            'query': query,\n",
    "                            'rank': result.get('rank'),\n",
    "                            'title': result.get('title', '').strip(),\n",
    "                            'link': url,\n",
    "                            'snippet': result.get('snippet', '').replace('\\n', ' ').strip(),\n",
    "                            'page': page + 1\n",
    "                        })\n",
    "                        new_results += 1\n",
    "                \n",
    "                print(f\"Page {page + 1}: {len(organic_results)} results | {new_results} new\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page + 1}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Save to CSV\n",
    "    if all_results:\n",
    "        fieldnames = ['query', 'rank', 'title', 'link', 'snippet', 'page']\n",
    "        \n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_results)\n",
    "            \n",
    "        print(f\"\\nSuccess! Saved {len(all_results)} unique results to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No results to save\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting Google searches for {len(QUERIES)} queries\")\n",
    "    fetch_and_save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f57ed2-46a5-4c94-8b26-26d6308aaad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 for query: 'administrative service' (Country: PH)...\n",
      "Scraping page 2 for query: 'administrative service' (Country: PH)...\n",
      "Scraping page 1 for query: 'natural disaster' (Country: PH)...\n",
      "Scraping page 2 for query: 'natural disaster' (Country: PH)...\n",
      "Scraping page 1 for query: 'disaster relief' (Country: PH)...\n",
      "Scraping page 2 for query: 'disaster relief' (Country: PH)...\n",
      "Scraping page 1 for query: 'aid delivery' (Country: PH)...\n",
      "Scraping page 2 for query: 'aid delivery' (Country: PH)...\n",
      "Scraping page 1 for query: 'government protection' (Country: PH)...\n",
      "Scraping page 2 for query: 'government protection' (Country: PH)...\n",
      "Scraping page 1 for query: 'capacity building program' (Country: PH)...\n",
      "Scraping page 2 for query: 'capacity building program' (Country: PH)...\n",
      "Scraping page 1 for query: 'resilience training' (Country: PH)...\n",
      "Scraping page 2 for query: 'resilience training' (Country: PH)...\n",
      "Scraping page 1 for query: 'local adaptation efforts' (Country: PH)...\n",
      "Scraping page 2 for query: 'local adaptation efforts' (Country: PH)...\n",
      "Scraping page 1 for query: 'municipal policy' (Country: PH)...\n",
      "Scraping page 2 for query: 'municipal policy' (Country: PH)...\n",
      "Scraping page 1 for query: 'weather warning' (Country: PH)...\n",
      "Scraping page 2 for query: 'weather warning' (Country: PH)...\n",
      "Scraping page 1 for query: 'rural improvement' (Country: PH)...\n",
      "Scraping page 2 for query: 'rural improvement' (Country: PH)...\n",
      "Scraping page 1 for query: 'regional law' (Country: PH)...\n",
      "Scraping page 2 for query: 'regional law' (Country: PH)...\n",
      "Scraping page 1 for query: 'provincial office' (Country: PH)...\n",
      "Scraping page 2 for query: 'provincial office' (Country: PH)...\n",
      "Scraping page 1 for query: 'official statistics' (Country: PH)...\n",
      "Scraping page 2 for query: 'official statistics' (Country: PH)...\n",
      "Scraping page 1 for query: 'climate report' (Country: PH)...\n",
      "Scraping page 2 for query: 'climate report' (Country: PH)...\n",
      "\n",
      "Total results collected: 8\n",
      "Saved to 'philippines.csv'\n"
     ]
    }
   ],
   "source": [
    "# COUNTRY-SPECIFIC (RE-TRY LATER - ADDED TIME DELAY AND HEADERS, CONSIDER ROTATING PROXIES (VPN?) AND HEADERS)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def extract_google_results(html_content):\n",
    "    \"\"\"Extract titles, links, descriptions, and website titles from Google search results.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    results = []\n",
    "    \n",
    "    # Find all search result items (Google's structure)\n",
    "    result_items = soup.find_all('div', class_='tF2Cxc')  # Updated for Google's current structure\n",
    "    \n",
    "    for item in result_items:\n",
    "        try:\n",
    "            # Extract title and link\n",
    "            title_tag = item.find('h3', class_='LC20lb')\n",
    "            title = title_tag.get_text() if title_tag else None\n",
    "            link = item.find('a')['href'] if item.find('a') else None\n",
    "            \n",
    "            # Extract description (Google's snippet)\n",
    "            description_tag = item.find('div', class_='VwiC3b')\n",
    "            description = description_tag.get_text() if description_tag else None\n",
    "            \n",
    "            # Extract website title (Google's URL display)\n",
    "            cite_tag = item.find('cite')\n",
    "            website_title = cite_tag.get_text() if cite_tag else None\n",
    "            \n",
    "            if title and link:  # Only add if we have basic info\n",
    "                results.append({\n",
    "                    'title': title.strip(),\n",
    "                    'link': link,\n",
    "                    'description': description.strip() if description else None,\n",
    "                    'website_title': website_title.strip() if website_title else None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing result: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def scrape_google_search(query, country_code=\"ph\", pages=2):\n",
    "    \"\"\"Scrape Google search results for a query with country-specific results.\"\"\"\n",
    "    base_url = \"https://www.google.com/search\"\n",
    "    all_results = []\n",
    "    \n",
    "    for page in range(pages):\n",
    "        try:\n",
    "            # Google pagination uses 'start' parameter (10 results per page)\n",
    "            start = page * 10\n",
    "            \n",
    "            params = {\n",
    "                \"q\": query,\n",
    "                \"gl\": country_code,  # Country code (e.g., \"us\", \"uk\", \"ca\")\n",
    "                \"start\": start\n",
    "            }\n",
    "            \n",
    "            headers_list = [\n",
    "                {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"},\n",
    "                {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\"},\n",
    "                {\"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"}\n",
    "            ]\n",
    "            \n",
    "            headers = random.choice(headers_list)\n",
    "\n",
    "            # proxy_list = [\n",
    "            #    \"http://123.456.789.1:8080\",\n",
    "            #    \"http://123.456.789.2:8080\",\n",
    "            #]\n",
    "            \n",
    "            # proxy = {\"http\": random.choice(proxy_list), \"https\": random.choice(proxy_list)}\n",
    "            # response = requests.get(base_url, headers=headers, proxies=proxy)\n",
    "            \n",
    "            print(f\"Scraping page {page + 1} for query: '{query}' (Country: {country_code.upper()})...\")\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            page_results = extract_google_results(response.text)\n",
    "            for result in page_results:\n",
    "                result['query'] = query  # Add the search query to each result\n",
    "                result['country'] = country_code.upper()  # Add country code\n",
    "            all_results.extend(page_results)\n",
    "            \n",
    "            # Random delay to avoid rate limiting\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of queries to search\n",
    "    queries = [\n",
    "        \"administrative service\",\n",
    "          \"natural disaster\",\n",
    "          \"disaster relief\",\n",
    "          \"aid delivery\",\n",
    "          \"government protection\",\n",
    "          \"capacity building program\",\n",
    "          \"resilience training\",\n",
    "          \"local adaptation efforts\",\n",
    "          \"municipal policy\",\n",
    "          \"weather warning\",\n",
    "          \"rural improvement\",\n",
    "          \"regional law\",\n",
    "          \"provincial office\",\n",
    "          \"official statistics\",\n",
    "          \"climate report\"\n",
    "    ]\n",
    "    \n",
    "    # Country code (e.g., \"us\", \"uk\", \"in\", \"au\")\n",
    "    country = \"ph\"\n",
    "    \n",
    "    # Scrape 10 pages per query\n",
    "    all_results = []\n",
    "    for query in queries:\n",
    "        search_results = scrape_google_search(query, country_code=country, pages=2)\n",
    "        all_results.extend(search_results)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"philippines.csv\", index=False)\n",
    "    print(f\"\\nTotal results collected: {len(df)}\")\n",
    "    print(\"Saved to 'philippines.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
